{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Job Vacancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "En este proyecto, crearemos un raspador web para extraer listados de empleos de una plataforma de búsqueda de empleo popular. Extraeremos títulos de puestos, empresas, ubicaciones, descripciones de puestos y otra información relevante. Estos son los pasos principales que seguiremos en este proyecto:\n",
    "\n",
    "1. Configurar nuestro entorno de desarrollo.\n",
    "2. Comprender los conceptos básicos del web scraping.\n",
    "3. Analizar la estructura del sitio web de nuestra plataforma de búsqueda de empleo.\n",
    "4. Escribir el código Python para extraer datos laborales de nuestra plataforma de búsqueda de empleo.\n",
    "    2. Guardar los datos en un archivo CSV.\n",
    "    3. Probar nuestro web scraper y refinar nuestro código según sea necesario.\n",
    "\n",
    "#### Requisitos previos\n",
    "\n",
    "Antes de comenzar este proyecto, debe tener algunos conocimientos básicos de programación en Python y estructura HTML. Además, es posible que desee utilizar los siguientes paquetes en su entorno Python:\n",
    "\n",
    "- `requests`\n",
    "- `BeautifulSoup`\n",
    "- `csv`\n",
    "- `datetime`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Paso 1: Configurar nuestro entorno de desarrollo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.2.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (0.10.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (2.23.0)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.11.2-py3-none-any.whl (7.2 MB)\n",
      "     |████████████████████████████████| 7.2 MB 36.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "     |████████████████████████████████| 400 kB 113.0 MB/s            \n",
      "\u001b[?25hCollecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "     |████████████████████████████████| 167 kB 117.0 MB/s            \n",
      "\u001b[?25hCollecting urllib3[socks]<3,>=1.26\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "     |████████████████████████████████| 124 kB 119.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Collecting attrs>=20.1.0\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "     |████████████████████████████████| 63 kB 9.8 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers in /opt/conda/lib/python3.7/site-packages (from trio~=0.17->selenium) (2.1.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting urllib3[socks]<3,>=1.26\n",
      "  Downloading urllib3-2.0.6-py3-none-any.whl (123 kB)\n",
      "     |████████████████████████████████| 123 kB 116.2 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-2.0.5-py3-none-any.whl (123 kB)\n",
      "     |████████████████████████████████| 123 kB 117.7 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "     |████████████████████████████████| 123 kB 119.8 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "     |████████████████████████████████| 123 kB 123.3 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "     |████████████████████████████████| 123 kB 127.5 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "     |████████████████████████████████| 144 kB 128.5 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "     |████████████████████████████████| 143 kB 17.7 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "     |████████████████████████████████| 143 kB 124.6 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.17-py2.py3-none-any.whl (143 kB)\n",
      "     |████████████████████████████████| 143 kB 123.1 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "     |████████████████████████████████| 143 kB 109.9 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "     |████████████████████████████████| 140 kB 120.4 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "     |████████████████████████████████| 140 kB 47.4 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "     |████████████████████████████████| 140 kB 124.6 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     |████████████████████████████████| 140 kB 91.2 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
      "     |████████████████████████████████| 139 kB 104.6 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n",
      "     |████████████████████████████████| 139 kB 102.1 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "     |████████████████████████████████| 138 kB 122.3 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "     |████████████████████████████████| 138 kB 60.9 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "     |████████████████████████████████| 138 kB 109.6 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "     |████████████████████████████████| 138 kB 118.4 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n",
      "     |████████████████████████████████| 138 kB 66.3 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "     |████████████████████████████████| 153 kB 105.6 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.3-py2.py3-none-any.whl (137 kB)\n",
      "     |████████████████████████████████| 137 kB 65.3 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "     |████████████████████████████████| 136 kB 125.2 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.1-py2.py3-none-any.whl (136 kB)\n",
      "     |████████████████████████████████| 136 kB 121.9 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.26.0-py2.py3-none-any.whl (136 kB)\n",
      "     |████████████████████████████████| 136 kB 119.4 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of urllib3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "     |████████████████████████████████| 127 kB 114.5 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "     |████████████████████████████████| 127 kB 123.0 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "     |████████████████████████████████| 126 kB 122.1 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
      "     |████████████████████████████████| 125 kB 123.8 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.25.7-py2.py3-none-any.whl (125 kB)\n",
      "     |████████████████████████████████| 125 kB 105.5 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.25.6-py2.py3-none-any.whl (125 kB)\n",
      "     |████████████████████████████████| 125 kB 110.0 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.25.5-py2.py3-none-any.whl (125 kB)\n",
      "     |████████████████████████████████| 125 kB 110.3 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of urllib3 to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
      "     |████████████████████████████████| 125 kB 106.4 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.25.3-py2.py3-none-any.whl (150 kB)\n",
      "     |████████████████████████████████| 150 kB 106.7 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.25.2-py2.py3-none-any.whl (150 kB)\n",
      "     |████████████████████████████████| 150 kB 71.5 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "     |████████████████████████████████| 118 kB 101.3 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.24.2-py2.py3-none-any.whl (131 kB)\n",
      "     |████████████████████████████████| 131 kB 103.8 MB/s            \n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading urllib3-1.24.1-py2.py3-none-any.whl (118 kB)\n",
      "     |████████████████████████████████| 118 kB 110.6 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.24-py2.py3-none-any.whl (117 kB)\n",
      "     |████████████████████████████████| 117 kB 117.5 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.23-py2.py3-none-any.whl (133 kB)\n",
      "     |████████████████████████████████| 133 kB 115.4 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.22-py2.py3-none-any.whl (132 kB)\n",
      "     |████████████████████████████████| 132 kB 112.9 MB/s            \n",
      "\u001b[?25h  Downloading urllib3-1.21.1-py2.py3-none-any.whl (131 kB)\n",
      "     |████████████████████████████████| 131 kB 81.1 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of trio-websocket to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.4-py3-none-any.whl (17 kB)\n",
      "INFO: pip is looking at multiple versions of trio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.1-py3-none-any.whl (399 kB)\n",
      "     |████████████████████████████████| 399 kB 105.0 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy>=1.0.1\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "     |████████████████████████████████| 38.1 MB 117.0 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "     |████████████████████████████████| 508 kB 75.2 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of python-dateutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "     |████████████████████████████████| 229 kB 118.3 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pyparsing to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "     |████████████████████████████████| 104 kB 78.4 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy>=1.13.3\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     |████████████████████████████████| 15.7 MB 102.8 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of kiwisolver to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 108.0 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of idna to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     |████████████████████████████████| 58 kB 32.4 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of cycler to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "INFO: pip is looking at multiple versions of chardet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     |████████████████████████████████| 133 kB 112.1 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of certifi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "     |████████████████████████████████| 162 kB 105.7 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of selenium to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.11.1-py3-none-any.whl (7.2 MB)\n",
      "     |████████████████████████████████| 7.2 MB 51.3 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.11.0-py3-none-any.whl (7.2 MB)\n",
      "     |████████████████████████████████| 7.2 MB 91.9 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.10.0-py3-none-any.whl (6.7 MB)\n",
      "     |████████████████████████████████| 6.7 MB 88.2 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.9.1-py3-none-any.whl (6.6 MB)\n",
      "     |████████████████████████████████| 6.6 MB 90.4 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.9.0-py3-none-any.whl (6.5 MB)\n",
      "     |████████████████████████████████| 6.5 MB 98.8 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.8.3-py3-none-any.whl (6.5 MB)\n",
      "     |████████████████████████████████| 6.5 MB 102.0 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.8.2-py3-none-any.whl (6.9 MB)\n",
      "     |████████████████████████████████| 6.9 MB 101.4 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of selenium to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading selenium-4.8.1-py3-none-any.whl (6.9 MB)\n",
      "     |████████████████████████████████| 6.9 MB 72.3 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.8.0-py3-none-any.whl (6.3 MB)\n",
      "     |████████████████████████████████| 6.3 MB 108.9 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.7.2-py3-none-any.whl (6.3 MB)\n",
      "     |████████████████████████████████| 6.3 MB 107.4 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.7.1-py3-none-any.whl (6.3 MB)\n",
      "     |████████████████████████████████| 6.3 MB 78.7 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.7.0-py3-none-any.whl (6.3 MB)\n",
      "     |████████████████████████████████| 6.3 MB 88.0 MB/s            \n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading selenium-4.6.1-py3-none-any.whl (6.0 MB)\n",
      "     |████████████████████████████████| 6.0 MB 55.1 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.6.0-py3-none-any.whl (5.2 MB)\n",
      "     |████████████████████████████████| 5.2 MB 53.1 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.5.0-py3-none-any.whl (995 kB)\n",
      "     |████████████████████████████████| 995 kB 107.5 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.4.3-py3-none-any.whl (985 kB)\n",
      "     |████████████████████████████████| 985 kB 109.4 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.4.2-py3-none-any.whl (985 kB)\n",
      "     |████████████████████████████████| 985 kB 100.5 MB/s            \n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "     |████████████████████████████████| 149 kB 114.5 MB/s            \n",
      "\u001b[?25hCollecting selenium\n",
      "  Downloading selenium-4.4.1-py3-none-any.whl (985 kB)\n",
      "     |████████████████████████████████| 985 kB 105.5 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.4.0-py3-none-any.whl (985 kB)\n",
      "     |████████████████████████████████| 985 kB 101.7 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
      "     |████████████████████████████████| 981 kB 102.4 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.2.0-py3-none-any.whl (983 kB)\n",
      "     |████████████████████████████████| 983 kB 100.3 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.1.5-py3-none-any.whl (979 kB)\n",
      "     |████████████████████████████████| 979 kB 106.1 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.1.4-py3-none-any.whl (979 kB)\n",
      "     |████████████████████████████████| 979 kB 77.4 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.1.3-py3-none-any.whl (968 kB)\n",
      "     |████████████████████████████████| 968 kB 109.6 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.1.2-py3-none-any.whl (963 kB)\n",
      "     |████████████████████████████████| 963 kB 75.6 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.1.1-py3-none-any.whl (963 kB)\n",
      "     |████████████████████████████████| 963 kB 108.1 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n",
      "     |████████████████████████████████| 958 kB 68.8 MB/s            \n",
      "\u001b[?25h  Downloading selenium-4.0.0-py3-none-any.whl (954 kB)\n",
      "     |████████████████████████████████| 954 kB 71.2 MB/s            \n",
      "\u001b[?25h  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "     |████████████████████████████████| 904 kB 102.4 MB/s            \n",
      "\u001b[?25hInstalling collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib seaborn requests selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Paso 2: Comprender los conceptos básicos del web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conceptos Básicos de Web Scraping\r\n",
    "\r\n",
    "Aquí tienes los conceptos básicos de **web scraping** de manera general:\r\n",
    "\r\n",
    "1. **HTTP Requests**: Web scraping comienza con el envío de solicitudes HTTP (GET o POST) al servidor web para obtener el contenido de una página. Las bibliotecas de Python como `requests` permiten hacer solicitudes y obtener el HTML de la página.\r\n",
    "\r\n",
    "2. **Parsing HTML**: Una vez que se obtiene el HTML, es necesario analizarlo (parsing) para extraer los datos específicos. Esto implica navegar y seleccionar elementos HTML como etiquetas, clases y atributos, utilizando bibliotecas como **BeautifulSoup** o **lxml**.\r\n",
    "\r\n",
    "3. **XPath y Selectores CSS**: XPath y selectores CSS son métodos para identificar elementos específicos en un documento HTML. XPath es útil para seleccionar elementos a través de rutas, mientras que los selectores CSS permiten especificar etiquetas y clases directamente, por ejemplo, `.class-name` o `#id`.\r\n",
    "\r\n",
    "4. **Navegación en el DOM**: El DOM (Document Object Model) es la estructura jerárquica del HTML. Comprender la estructura del DOM es clave para localizar y extraer datos con precisión, ya que los datos se encuentran en nodos que se pueden recorrer de arriba hacia abajo o de izquierda a derecha.\r\n",
    "\r\n",
    "5. **Manejo de Datos Dinámicos**: Algunos sitios generan contenido dinámicamente con JavaScript, lo que significa que los datos no están disponibles en el HTML inicial. Para ello, herramientas como **Selenium** o **Playwright** emulan la interacción con el navegador y permiten obtener el HTML final, incluido el contenido dinámico.\r\n",
    "\r\n",
    "6. **Exportación y Almacenamiento de Datos**: Los datos extraídos suelen almacenarse en formatos estructurados como CSV, JSON o bases de datos SQL/NoSQL, lo que permite manipularlos y analizarlos fácilmente.\r\n",
    "\r\n",
    "7. **Respeto por las Políticas de Scraping (Robots.txt)**: Es fundamental respetar las reglas de `robots.txt` de cada sitio web, que especifican las páginas y los límites de frecuencia permitidos para los bots, y tener en cuenta el buen uso de las tasas de solicitud para evitar sobrecargar el servidor.\r\n",
    "\r\n",
    "Estos conceptos forman la base del web scraping y ayudan a estructurar un proyecto de scraping de manera eficiente y respetuosa con las políticas del sitio web.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Analizar la estructura del sitio web de nuestra plataforma de búsqueda de empleo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "En este caso, se ha seleccionado el buscador de empleo **Computrabajo** en el país de Colombia.\r\n",
    "\r\n",
    "**Enlace**: [https://co.computrabajo.com](https://co.computrabajo.com)\r\n",
    "\r\n",
    "Para el análisis HTML, es importante comprender cómo funciona el buscador y cómo organiza y categoriza las vacantes en su página web. Esto se realiza a través del estudio de la estructura HTML, identificando los `<div>` correspondientes a los elementos deseados, que en este caso son las cajas que muestran las vacantes de empleo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consideraciones\n",
    "\n",
    "**Para mejorar:** Es posible crear un archivo para cada vacante que se actualice. Actualmente, solo se cuenta con las primeras 20 vacantes, es decir, de la primera página. Se podría optimizar el código para que busque en todo el número de vacantes y colocar la fecha de la consulta (código: implementar la navegación a las demás páginas).\n",
    "\n",
    "Además, sería útil incluir las descripciones de cada empleo, ya que a menudo contienen información importante. Para lograr esto, se debe acceder a cada vacante, lo que implica cambiar la URL. También sería recomendable crear una base de datos para las vacantes seleccionadas, ya que no todas están disponibles en la plataforma de búsqueda. Esto se podría lograr extrayendo los nombres de los puestos de empleo directamente del buscador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Escribir el código Python para extraer datos laborales de nuestra plataforma de búsqueda de empleo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Solicitud de empleo y creación de URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para solicitar el trabajo al usuario\n",
    "def enter_job_url():\n",
    "    job = input('Ingresa el trabajo que quieres buscar: ').strip()\n",
    "    listjob = job.split()\n",
    "    url = 'https://co.computrabajo.com/' + 'trabajo-de-' + \"-\".join(listjob)\n",
    "    print(f\"URL a buscar: {url}\")\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Solicitud HTML al sitio: Extracción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa el trabajo que quieres buscar:  analista de datos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL a buscar: https://co.computrabajo.com/trabajo-de-analista-de-datos\n",
      "Total de ofertas: 1391\n",
      "Datos guardados en Job_Offers.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def analysis_HTML():\n",
    "    # Obtén la URL para buscar el trabajo\n",
    "    url = enter_job_url()\n",
    "\n",
    "    # Configurar Selenium y abrir la página web\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Esperar explícitamente hasta que los elementos carguen (máximo 20 segundos)\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'box_offer'))\n",
    "        )\n",
    "    except:\n",
    "        print(\"No se pudo cargar la página a tiempo.\")\n",
    "        driver.quit()\n",
    "        return\n",
    "\n",
    "    # Extraer ofertas\n",
    "    def extract_offers():\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extraer el número total de ofertas desde el h1 dentro del div box_title\n",
    "        num_offers_element = soup.select_one('div.box_title h1.title_page span.fwB')\n",
    "\n",
    "        if num_offers_element:\n",
    "            # Obtener el texto dentro del span y procesarlo\n",
    "            num_offers_text = num_offers_element.text.strip()\n",
    "            # print(f\"Texto extraído del span: '{num_offers_text}'\")  # Imprimir el texto para depuración\n",
    "    \n",
    "            try:\n",
    "                # Intentar convertir a número entero\n",
    "                total_offers = int(num_offers_text.replace('.', ''))\n",
    "                print(f\"Total de ofertas: {total_offers}\")\n",
    "            except ValueError:\n",
    "                print(f\"Error al convertir a entero: '{num_offers_text}' no es un número válido.\")\n",
    "        else:\n",
    "            print(\"No se encontró el número de ofertas.\")\n",
    "\n",
    "        # Usar una lista para almacenar la información de las ofertas\n",
    "        job_list = []\n",
    "\n",
    "        # Iterar sobre los primeros 20 artículos\n",
    "        for i in range(20):\n",
    "            offer_tag = soup.find('article', {'data-lc': f'ListOffers-Score4-{i}'})\n",
    "            \n",
    "            if not offer_tag:\n",
    "                continue  # Si no encuentra la oferta, pasa a la siguiente\n",
    "\n",
    "            # Extraer título, enlace, empresa, ubicación, salario y tiempo de publicación\n",
    "            title_tag = offer_tag.find('h2', class_='fs18 fwB')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else 'No disponible'\n",
    "\n",
    "            link_tag = offer_tag.find('a', class_='js-o-link fc_base')\n",
    "            link = link_tag['href'] if link_tag else 'No disponible'\n",
    "\n",
    "            company_tag = offer_tag.find('a', class_='fc_base t_ellipsis')\n",
    "            company = company_tag.get_text(strip=True) if company_tag else 'No disponible'\n",
    "\n",
    "            location_tag = offer_tag.find('p', class_='fs16 fc_base mt5')\n",
    "            location = location_tag.get_text(strip=True) if location_tag else 'No disponible'\n",
    "\n",
    "            salary_tag = offer_tag.find('div', class_='fs13 mt15')\n",
    "            salary = salary_tag.get_text(strip=True) if salary_tag else 'No especificado'\n",
    "\n",
    "            time_posted_tag = offer_tag.find('p', class_='fs13 fc_aux mt15')\n",
    "            time_posted = time_posted_tag.get_text(strip=True) if time_posted_tag else 'No disponible'\n",
    "\n",
    "            # Agregar los datos a la lista de ofertas\n",
    "            job_list.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'company': company,\n",
    "                'location': location,\n",
    "                'salary': salary,\n",
    "                'time_posted': time_posted\n",
    "            })\n",
    "\n",
    "        return job_list\n",
    "\n",
    "    # B. Guardar datos en un archivo CSV\n",
    "    def save_csv():\n",
    "        offers = extract_offers()\n",
    "        if offers:\n",
    "            df = pd.DataFrame(offers)\n",
    "            df.to_csv('Job_Offers.csv', index=False)\n",
    "            print(\"Datos guardados en Job_Offers.csv\")\n",
    "        else:\n",
    "            print(\"No se encontraron ofertas para guardar.\")\n",
    "    \n",
    "    save_csv()\n",
    "    driver.quit()\n",
    "\n",
    "analysis_HTML()\n",
    "# C. Probar el programa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Análisis de las vacantes (a vuestro criterio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el CSV \n",
    "# Qué quiero analizar encontrar un (vs)  por ejemplo ¿qué compañia paga más?, ¿que tan cerca queda de mi hogar?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
