{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Job Vacancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "En este proyecto, crearemos un raspador web para extraer listados de empleos de una plataforma de búsqueda de empleo popular. Extraeremos títulos de puestos, empresas, ubicaciones, descripciones de puestos y otra información relevante. Estos son los pasos principales que seguiremos en este proyecto:\n",
    "\n",
    "1. Configurar nuestro entorno de desarrollo.\n",
    "2. Comprender los conceptos básicos del web scraping.\n",
    "3. Analizar la estructura del sitio web de nuestra plataforma de búsqueda de empleo.\n",
    "4. Escribir el código Python para extraer datos laborales de nuestra plataforma de búsqueda de empleo.\n",
    "    2. Guardar los datos en un archivo CSV.\n",
    "    3. Probar nuestro web scraper y refinar nuestro código según sea necesario.\n",
    "\n",
    "#### Requisitos previos\n",
    "\n",
    "Antes de comenzar este proyecto, debe tener algunos conocimientos básicos de programación en Python y estructura HTML. Además, es posible que desee utilizar los siguientes paquetes en su entorno Python:\n",
    "\n",
    "- `requests`\n",
    "- `BeautifulSoup`\n",
    "- `csv`\n",
    "- `datetime`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Paso 1: Configurar nuestro entorno de desarrollo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib seaborn requests selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Paso 2: Comprender los conceptos básicos del web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conceptos Básicos de Web Scraping\r\n",
    "\r\n",
    "Aquí tienes los conceptos básicos de **web scraping** de manera general:\r\n",
    "\r\n",
    "1. **HTTP Requests**: Web scraping comienza con el envío de solicitudes HTTP (GET o POST) al servidor web para obtener el contenido de una página. Las bibliotecas de Python como `requests` permiten hacer solicitudes y obtener el HTML de la página.\r\n",
    "\r\n",
    "2. **Parsing HTML**: Una vez que se obtiene el HTML, es necesario analizarlo (parsing) para extraer los datos específicos. Esto implica navegar y seleccionar elementos HTML como etiquetas, clases y atributos, utilizando bibliotecas como **BeautifulSoup** o **lxml**.\r\n",
    "\r\n",
    "3. **XPath y Selectores CSS**: XPath y selectores CSS son métodos para identificar elementos específicos en un documento HTML. XPath es útil para seleccionar elementos a través de rutas, mientras que los selectores CSS permiten especificar etiquetas y clases directamente, por ejemplo, `.class-name` o `#id`.\r\n",
    "\r\n",
    "4. **Navegación en el DOM**: El DOM (Document Object Model) es la estructura jerárquica del HTML. Comprender la estructura del DOM es clave para localizar y extraer datos con precisión, ya que los datos se encuentran en nodos que se pueden recorrer de arriba hacia abajo o de izquierda a derecha.\r\n",
    "\r\n",
    "5. **Manejo de Datos Dinámicos**: Algunos sitios generan contenido dinámicamente con JavaScript, lo que significa que los datos no están disponibles en el HTML inicial. Para ello, herramientas como **Selenium** o **Playwright** emulan la interacción con el navegador y permiten obtener el HTML final, incluido el contenido dinámico.\r\n",
    "\r\n",
    "6. **Exportación y Almacenamiento de Datos**: Los datos extraídos suelen almacenarse en formatos estructurados como CSV, JSON o bases de datos SQL/NoSQL, lo que permite manipularlos y analizarlos fácilmente.\r\n",
    "\r\n",
    "7. **Respeto por las Políticas de Scraping (Robots.txt)**: Es fundamental respetar las reglas de `robots.txt` de cada sitio web, que especifican las páginas y los límites de frecuencia permitidos para los bots, y tener en cuenta el buen uso de las tasas de solicitud para evitar sobrecargar el servidor.\r\n",
    "\r\n",
    "Estos conceptos forman la base del web scraping y ayudan a estructurar un proyecto de scraping de manera eficiente y respetuosa con las políticas del sitio web.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Analizar la estructura del sitio web de nuestra plataforma de búsqueda de empleo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "En este caso, se ha seleccionado el buscador de empleo **Computrabajo** en el país de Colombia.\r\n",
    "\r\n",
    "**Enlace**: [https://co.computrabajo.com](https://co.computrabajo.com)\r\n",
    "\r\n",
    "Para el análisis HTML, es importante comprender cómo funciona el buscador y cómo organiza y categoriza las vacantes en su página web. Esto se realiza a través del estudio de la estructura HTML, identificando los `<div>` correspondientes a los elementos deseados, que en este caso son las cajas que muestran las vacantes de empleo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consideraciones\n",
    "\n",
    "**Para mejorar:** Es posible crear un archivo para cada vacante que se actualice. Actualmente, solo se cuenta con las primeras 20 vacantes, es decir, de la primera página. Se podría optimizar el código para que busque en todo el número de vacantes y colocar la fecha de la consulta (código: implementar la navegación a las demás páginas).\n",
    "\n",
    "Además, sería útil incluir las descripciones de cada empleo, ya que a menudo contienen información importante. Para lograr esto, se debe acceder a cada vacante, lo que implica cambiar la URL. También sería recomendable crear una base de datos para las vacantes seleccionadas, ya que no todas están disponibles en la plataforma de búsqueda. Esto se podría lograr extrayendo los nombres de los puestos de empleo directamente del buscador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Escribir el código Python para extraer datos laborales de nuestra plataforma de búsqueda de empleo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Solicitud de empleo y creación de URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para solicitar el trabajo al usuario\n",
    "def enter_job_url():\n",
    "    job = input('Ingresa el trabajo que quieres buscar: ').strip()\n",
    "    listjob = job.split()\n",
    "    url = 'https://co.computrabajo.com/' + 'trabajo-de-' + \"-\".join(listjob)\n",
    "    print(f\"URL a buscar: {url}\")\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Solicitud HTML al sitio: Extracción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa el trabajo que quieres buscar:  analista de datos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL a buscar: https://co.computrabajo.com/trabajo-de-analista-de-datos\n",
      "Total de ofertas: 1391\n",
      "Datos guardados en Job_Offers.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def analysis_HTML():\n",
    "    # Obtén la URL para buscar el trabajo\n",
    "    url = enter_job_url()\n",
    "\n",
    "    # Configurar Selenium y abrir la página web\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Esperar explícitamente hasta que los elementos carguen (máximo 20 segundos)\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'box_offer'))\n",
    "        )\n",
    "    except:\n",
    "        print(\"No se pudo cargar la página a tiempo.\")\n",
    "        driver.quit()\n",
    "        return\n",
    "\n",
    "    # Extraer ofertas\n",
    "    def extract_offers():\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extraer el número total de ofertas desde el h1 dentro del div box_title\n",
    "        num_offers_element = soup.select_one('div.box_title h1.title_page span.fwB')\n",
    "\n",
    "        if num_offers_element:\n",
    "            # Obtener el texto dentro del span y procesarlo\n",
    "            num_offers_text = num_offers_element.text.strip()\n",
    "            # print(f\"Texto extraído del span: '{num_offers_text}'\")  # Imprimir el texto para depuración\n",
    "    \n",
    "            try:\n",
    "                # Intentar convertir a número entero\n",
    "                total_offers = int(num_offers_text.replace('.', ''))\n",
    "                print(f\"Total de ofertas: {total_offers}\")\n",
    "            except ValueError:\n",
    "                print(f\"Error al convertir a entero: '{num_offers_text}' no es un número válido.\")\n",
    "        else:\n",
    "            print(\"No se encontró el número de ofertas.\")\n",
    "\n",
    "        # Usar una lista para almacenar la información de las ofertas\n",
    "        job_list = []\n",
    "\n",
    "        # Iterar sobre los primeros 20 artículos\n",
    "        for i in range(20):\n",
    "            offer_tag = soup.find('article', {'data-lc': f'ListOffers-Score4-{i}'})\n",
    "            \n",
    "            if not offer_tag:\n",
    "                continue  # Si no encuentra la oferta, pasa a la siguiente\n",
    "\n",
    "            # Extraer título, enlace, empresa, ubicación, salario y tiempo de publicación\n",
    "            title_tag = offer_tag.find('h2', class_='fs18 fwB')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else 'No disponible'\n",
    "\n",
    "            link_tag = offer_tag.find('a', class_='js-o-link fc_base')\n",
    "            link = link_tag['href'] if link_tag else 'No disponible'\n",
    "\n",
    "            company_tag = offer_tag.find('a', class_='fc_base t_ellipsis')\n",
    "            company = company_tag.get_text(strip=True) if company_tag else 'No disponible'\n",
    "\n",
    "            location_tag = offer_tag.find('p', class_='fs16 fc_base mt5')\n",
    "            location = location_tag.get_text(strip=True) if location_tag else 'No disponible'\n",
    "\n",
    "            salary_tag = offer_tag.find('div', class_='fs13 mt15')\n",
    "            salary = salary_tag.get_text(strip=True) if salary_tag else 'No especificado'\n",
    "\n",
    "            time_posted_tag = offer_tag.find('p', class_='fs13 fc_aux mt15')\n",
    "            time_posted = time_posted_tag.get_text(strip=True) if time_posted_tag else 'No disponible'\n",
    "\n",
    "            # Agregar los datos a la lista de ofertas\n",
    "            job_list.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'company': company,\n",
    "                'location': location,\n",
    "                'salary': salary,\n",
    "                'time_posted': time_posted\n",
    "            })\n",
    "\n",
    "        return job_list\n",
    "\n",
    "    # B. Guardar datos en un archivo CSV\n",
    "    def save_csv():\n",
    "        offers = extract_offers()\n",
    "        if offers:\n",
    "            df = pd.DataFrame(offers)\n",
    "            df.to_csv('Job_Offers.csv', index=False)\n",
    "            print(\"Datos guardados en Job_Offers.csv\")\n",
    "        else:\n",
    "            print(\"No se encontraron ofertas para guardar.\")\n",
    "    \n",
    "    save_csv()\n",
    "    driver.quit()\n",
    "\n",
    "analysis_HTML()\n",
    "# C. Probar el programa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Análisis de las vacantes (a vuestro criterio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el CSV \n",
    "# Qué quiero analizar encontrar un (vs)  por ejemplo ¿qué compañia paga más?, ¿que tan cerca queda de mi hogar?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
